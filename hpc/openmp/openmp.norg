* OpenMP

** Moore's law

   Goordon Moore came up with an empirical law that stated that every 2 years, 
   the number of transistors that on a device would double. 

   As a consequence, manufactures would pack more transistors into a single 
   core and increase performance.

   Many years later, Moore's law is still working. Over the years we see that 
   performance keeps increasing.

   Because of this, people were trained to expect that performance would come 
   automatically from hardware at no cost. This was known as the "Free Ride" 
   era since every two years the performance of any application would double 
   for "free" as long as one would get the latest processor.

   As a consequence, many programmers were not taught to explicitly think about 
   performance.

   However, something happened that stopped this trend. 

*** The power wall problem

    If we plot the power vs. performance, it can be seen that as performance 
    increases, the power consumption increases almost quadratically. 

    This meant that the trend of packing more transistors in a single core to 
    increase performance would be unsustainable in the long term, which meant 
    the end of the Free Ride era.

    This caused manufactures to try to optimize for power as well as 
    performance. They began to manufacture cores with less transistors and 
    therefore less frequency (i.e. a simpler architecture). However, they began
    to put more cores into a single device (CPU)!

    This was the start of the multicore era and the shared memory paradigm where 
    cores would need to share the RAM (and other resources) among themselves 
    efficiently in order to obtain increased performance.

*** Some physics

    Below we explain why power decreases as you pack more cores together.

    The capacitance C of a core is defined as $C = q*V$ where q is the charge 
    and V is the voltage. Work is defined as $W = V*q$. Therefore, this implies 
    that $W = CV^2$. Since power p is defined as $P = W*f$ where $f$ is the 
    frequency, we obtain that $P = C*V^2*f$.

    So power scales linearly in capacitance and frequency, but quadratically in 
    voltage. 

    So given a single core with a fixed frequency and desired ouput, we have 
    some fixed power consumption. 

    However, taking two cores with half the frequency (capacitance doubles) and 
    same output, we find that power goes down!

    So we are able to obtain the same output but with less power. This means 
    we can keep increasing output while optimizing for power. 

    So this is why parallel computing will be the new paradigm.

*** From Shared Memory to Distributed Memory

    As more and more cores began to be packed inside a single CPU, a new problem
    arose: the classic architecture which relied on the front side bus and the 
    northbridge to communicate between CPU and RAM became a major bottleneck
    since all the cores were fighting for the same resource. 

    For this reason, the northbridge was moved inside the CPU and more lanes 
    were introduced that connected to RAM. 

    Finally, to scale even more, multi-socket motherboards were introduced which 
    could use multiple CPU's (a.k.a sockets). The sockets were all connected 
    together through some technology such as Quick Path Interconnect, which 
    effectively made all memory visible to all the sockets.

    This introduced a non-uniform memory (NUMA) access paradigm since each 
    socket had a faster access time to its closest memory bank, but could still 
    access other banks which were farther away, at a higher cost. 

    Lastly, to scale even further, many of these "nodes", a.k.a a multi-socket
    device, were connected together by a very fast network (internet). This is 
    the so called "distributed memory" paradigm, where nodes need to explicitly 
    send messages (message passing) to each other throught the network. 
    To make this paradigm usable, the network needs to be very fast, and since 
    the protocol is less strict than http or https, typically the bandwith of 
    these networks are much higher than standard internet connections with 
    bandwiths of 100 Gbit/s. 

    A last important note is that the shared memory and distributed memory 
    paradigms are a concept at software level and can be completely detached 
    from the physical hardware level. 

    In other words, it is the */way/* we code that changes, and this is 
    independent of the hardware we have. In fact, we can do message passing on 
    a single node machine such as our laptop! In this case, each of our cores 
    will act as if they were a separate process with their own memory, and the 
    programmer will need to explicitly send messages between cores in the 
    code. 

    *How* this happens will then be determined at hardware level. In the 
    case of a multi-node machine, it will be done through the network, while in 
    the case of a single-node machine such as a laptop, it will be done through 
    normal data sharing which will be much faster.

    Viceversa, there exist some libraries which enable one to code in shared 
    memory while on a multi-node machine. Of course, the library will do some
    message passing under the hood, but the programmer */thinks/* in shared 
    memory.

    This is a very useful concept which must be clear - the separation between 
    software level constructs and hardware level requirements.

** OpenMP 

   A CPU can have many threads which are running concurrently.

   The fundamental idea of OpenMP is to enable an interface for managing threads
   which otherwise interleave each other.

   OpenMP instead enables the creation of threads which all execute the same 
   block of code (called a structured block). This is done within a fork-join 
   framework where the master thread (thread 0), upon finding directives of 
   OpenMP for thread creation, "forks" (creates) many threads which execute the 
   block of code in a concurrent and parallel fashion. After these threads exit 
   the block of code, they are once again joined to the master thread.

   It is important to understand that threads share all the memory before the 
   their creation, and additionally have their own private memory!

   Something to keep in mind is that we must be careful with race conditions.

*** Concurrency and Parallelism

    - Concurrency: a property of a system when multiple things *may* be happening 
      at the same time.
    - Paralellism: a subset of concurrency. We are taking some of the 
      concurrency and actually executing at the same time.

    *insert conc_vs_par.jpg*

    So we may have concurrent threads which are not parallel since they are just 
    swapping in and out of execution. 

    On the other hand, parallel threads are executing instructions at the same 
    time.

    The idea is that we need to expose the concurrency of a problem and then map 
    it to processing units so they execute at the same time (paralellism).

*** Description OpenMP

    OpenMP is a set of compiler directives and runtime libraries to be able to 
    run code in parallel.

    It makes it as easy as possible to write a parallel program.

    At the low level, it assumes shared address space. 
    On top of it, we have an OS that supports a multi-threaded approach.

    Finally, at the top is the programming layer, which is all the commands, 
    environment variables, and OMP libraries that we will describe to enable 
    parallel execution.

*** Core Syntax

     OpenMP (OMP) consists of compiler directives of the form

     @code c
     // this library includes functions and variables that we need to 
     // run openmp
     #include <omp.h>
     #pragma omp construct [clause1] [clause2] ...

     @end
     
     Most of the constructs of OMP apply to a "structured block", which is 
     simply a block of code with an entry point at the top and another at the 
     end. 

     Conceptually, our code will look something like this:

     @code c
     #include <omp.h>

     int main(){

        //
        // some serial code done by one thread
        //

        // Indicates the start of a parallel region where multiple threads will
        // be created and will execute the structured block of code inside {}

        #pragma omp parallel        
        {
             // some code block (a.k.a structured block).
             // multiple threads will execute this block of code 
             // in parallel.
        }                           

     }
     @end

     The #pragma omp parallel directive tells the compiler to spawn some threads 
     which will execute the code block. The number of threads depends on the 
     OS, however, we will talk about this in more detail later.

     OMP uses the master-fork-join paradigm, i.e. one in which a *master* thread 
     spawns some threads (a *fork*) which are *joined* at the end of a parallel 
     region.

     Each of the threads has a private stack frame, but share the stack of the 
     parent thread (also called the master thread). This means all of the 
     variables declared inside the code block are private to each thread, while 
     variables declared *before* the code block are visible by all the threads.
     This has important consequences in how we handle variables as we will 
     see later.
        
     Below is a very simple omp program.

     #tangle hello.v1.c
     @code c 
     #include <omp.h>
     #include <stdio.h>
     int main()
     {

         printf("I am the master thread and I will print only once!\n");
         #pragma omp parallel
         {
             printf("I am a lowly thread among many, hello!\n");
         }
     }
     @end

     We compile this program as "gcc -fopenmp hello.v1.c -o hello.v1", 
     to obtain the executable. Note that to use openmp, we have to compile 
     with the flag *-fopenmp*. 

     If we run the executable, we will see that the phrase hello is printed 
     multiple times in standard output (once by each thread) while the first 
     print statement is executed only once.

*** Some initial run-time commands

    Now we will describe some useful run-time commands for beginners.

    - `omp_set_num_threads(int number)` -> void : This function sets the number
    of threads that OpenMP will *try* to create. It is important to note that 
    it only asks for that number of threads, but the OS may give less to avoid 
    some crashes or resource overallocation.

    - `omp_get_num_threads()` -> int : Gives the total threads that have been 
    spawned. Useful to check how many threads I actually have.

    - `omp_get_thread_num()` -> int : Gives the id of the current thread that
    calls this function, i.e. if I have 5 threads and the third thread calls 
    this function, it will return 2 (note that we start counting from 0)

    - `omp_get_wtime()` -> double : return the number of seconds with respect 
    to some time in the past.

*** Assumed hardware/software

    Today, all machines are NUMA (non-uniform memory access) machines. Even a 
    single processor level, each core has its own L1 cache, therefore memory 
    access times are not uniform. 

    As we know a program has a stack, a heap, a text, and a data region. Modern 
    OS's decompose further into threads by fragmenting the stack. Each thread has 
    its private stack, which makes it cheap to switch between threads based on 
    context (this is handled by the OS). However, all threads share the heap, 
    text, and data regions. 

    So in summary, all the threads have their private stack, but share the heap, 
    which belongs to the process that is calling the program execution.

    *include threads_vs_process.jpg*

    Note that we can have more threads than cores! Nothing stops us from being 
    in this scenario. Furthermore, when a program is running, we must think of 
    all the  possible ways that the threads can be interleaved to give a correct 
    execution, since these may actually happen. This is why, in the program 
    below, the order of the output is random.

     #tangle hello.v2.c
     @code c 
     #include <omp.h>
     #include <stdio.h>
     int main()
     {

         #pragma omp parallel
         {
             int my_id = omp_get_thread_num();
             int num_threads = omp_get_num_threads();

             printf("Hi from thread %d of %d\n", my_id, num_threads);
         }
     }
     @end

    The order is random since the threads can interleave in any possible order.
    This concept will be important later!

    Furthermore, it is important to understand that threads communicate with 
    each other through shared variables.

*** Race Conditions

    Sometimes, threads share data in an unsafe way, i.e. a thread writes in
    a location (a variable) that another thread is trying to read. 

    This leads to a so called "race condition" because the outcome of the 
    program depends on what happened at that particular runtime (did the thread 
    write before the other one managed to read? or viceversa?).

    Race conditions are the biggest problem in parallel programming.

    How do we prevent race conditions? By controlling and organizing access to 
    shared variables. 
    We can achieve this through {# Synchronization}[synchronization]. 

    However, {# Synchronization}[synchronization] is very expensive and we must try to minimize it 
    as much as possible. We do this by being careful with how we organize our 
    code and data environment.

*** How to create threads in OpenMP

    As mentioned above, OMP uses a fork-join model. At the beginning, our program 
    has a single thread which we call the master thread. 
    Then it encounters a portion of execution where more threads can help out.  

    The master thread forks threads at that point. The master thread continues 
    and it has ID 0 while the other threads all have increasing ID's. This is 
    called a collection of threads. 

    When they all finish their work, they join back together again and we are 
    left with one thread. 

    This process can be repeated as much as needed, and we can even nest threads.

    This is done through the #pragma omp parallel construct that we saw before.
    And this is the *only* way we can create multiple threads using OMP.

    Take a look at the following code:

    @code c 
    int main(){
        double A[1000];
        // request 4 threads (OS will try to satisfy, not guaranteed)
        omp_set_num_threads(4);
        #pragma omp parallel
        {
            // each thread will run this block of code
            int ID = omp_get_thread_num();
            foo(ID,A);
        }
    }
    @end

    If we allocate data outside the #pragma omp parallel, it is in the heap, and 
    it is shared among all threads. 
    If I allocate the data inside the #pragma omp parallel, it is allocated in 
    the threads stack. So it is private.

    So in this case, all threads see the same copy of array A, and they all 
    call the function foo() with the same copy of A, but with their *own* 
    ID.

    If we were to print out the address of ID inside the code block, we would 
    see that they are all different. On the other hand, if we print the address 
    of A\[0\] from inside the code block, they all print the same address.

    But what does the compiler do? It takes the structured block, and packages 
    it into a function called a "thunk". Then it sets up the p-threads environment 
    and it launches each p-thread using a copy of the thunk.

    The advantage of using OMP is that we don't have to deal with the low level 
    interface of p-threads.

*** \[Exercise\] Hello World of Parallel Computing

    Below is a serial program that calculates pi.

    Using only the constructs seen so far, how would you parallelize it?

    #tangle serial_pi.c
    @code c 
    #include <stdio.h>
    static long num_steps = 100000;
    double step;
    int main()
    {
        int i;
        double x, pi, sum = 0.0;
        step = 1.0/(double)num_steps;
        for (i=0; i<num_steps;i++){
            x = (i+0.5)*step;
            sum += 4.0/(1.0+x*x);
        }
        pi = step * sum
    }
    @end





** False Sharing

   False sharing is an important problem we must always keep in mind to avoid
   serious performance hits. Basically, it occurs when we have a shared variable 
   among all threads. Suppose all threads want to read this variable, then, a copy 
   of the variable will be pulled in the cache of the core belonging to a specific
   thread. Suppose that all threads then want to write to this variable and change it. 
   They will each modify their local version, which in turn will activate a cache coherency
   protocol which ensures that the local copy of the variable in memory is always coherent.
   This means however, that we will have a lot moving back and forth of the variable
   among caches as it is updated. Thus performance will be terrible.

** Synchronization

   two types:
   - barrier
   - exclusion

*** barrier

    OMP provides functionality to ensure that all threads are synchronized at a barrier.
    This can be achieved by the directive:
    `#pragma omp barrier`

    Upon finding this directive, all threads will wait until all the threads have called
    the barrier and then keep going to the next block of code.

    // insert example

    Subtle point: By default, the barrier is inserted automatically after any worksharing
    construct, which we will talk about later. However, it is important to keep in mind
    that while the barrier is useful to make sure that all threads are synchronized at 
    some point, if one doesn't truly need the barrier, this can slow down performance.

*** exclusion

    OMP provides a few constructs for exclusion, and they vary in their granularity of 
    low level control that we can achieve.

    1. '#pragma omp critical'  : This directive ensures that only one thread at a time
    is executing the code found in the subsequent structured block. Note that all other
    threads will be waiting before this directive to be able to execute the code block.
    Therefore, if not needed, this could slow down performance. 

    2. '#pragma omp atomic' : This directive is similar to atomic, but is slightly more 
    low level in the sense that for simple operations, it can take advantage of some 
    hardware functionality for switching between threads. If this functionality is not 
    available, it will be identical to '#pragma omp critical'.

    3. Locks: Locks are the most low level directives that are offered by OMP. In a 
    few words, locks are variables of a type declared in <OMP.h> and they enable rich
    functionality:
    - omp_set_lock() : the thread that calls this function has the lock and can 
    execute the code requiring that lock 
    - omp_get_lock() : the thread that calls this function tries to obtain the 
    lock, and if it fails, it will wait until the lock is available.
    - omp_test_locl() : the thread will test whether the lock is free. If it isn't 
    it will keep on going to do other stuff instead of waiting.

    EXAMPLE 


** Worksharing constructs:

   OMP provides some worksharing construct. As an example, consider:

   @code c
   #pragma omp parallel{
   #pragma omp for

    }
   @end

   sections


** Reductions


* References 
  - An amazing {https://www.youtube.com/playlist?list=PLLbPZJxtMs4ZHSamRRYCtvowRS0qIwC-I}[tutorial] by Tim Mattson from Intel.
    He was one of the original developers of OpenMP and explains all the 
    concepts with amazing clarity.
  -- The repository of the {https://github.com/tgmattso/OpenMP_intro_tutorial}[course] with all the code and exercises he discusses
  - Luca Tornatore's slides on the OpenMP portion of the HPC course for UniTS. 
  -- {https://github.com/Foundations-of-HPC/Foundations_of_HPC_2020}[HPC 2020]
  -- {https://github.com/Foundations-of-HPC/Foundations_of_HPC_2021/tree/main/OpenMP}[HPC 2021]
  -- {https://github.com/Foundations-of-HPC/Foundations_of_HPC_2022/tree/main/Basic/OpenMP}[HPC 2022]
  - https://github.com/radix-io/hands-on/tree/main/mpi2tutorial/examples/life

