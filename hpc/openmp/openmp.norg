* OpenMP

** Moore's law

   Goordon Moore came up with an empirical law that stated that every 2 years, 
   the number of transistors that on a device would double. 

   As a consequence, manufactures would pack more transistors into a single 
   core and increase performance.

   Because of this, people were trained to expect that performance would come 
   automatically from hardware at no cost. This was known as the "Free Ride" 
   era since every two years the performance of any application would double 
   for "free" as long as one would get the latest processor.

   As a consequence, many programmers were not taught to explicitly think about 
   performance explicitly.

   However, something happened that stopped this trend. 

*** The power wall problem

    Plotting the power vs. performance, it can be seen that as performance 
    increases, the power consumption increases almost quadratically. This meant 
    that the trend of packing more transistors in a single core to increase 
    performance would be unsustainable in the long term, which meant the end of 
    the Free Ride era.

    This caused manufactures to try to optimize for power as well as 
    performance. They began to manufacture cores with less transistors and 
    therefore, less frequency and performance. However, they began to put more 
    cores into a single device!

    This was the start of the multicore era and the shared memory paradigm where 
    cores would need to share the RAM (and other resources) among themselves 
    efficiently to optimize performance.

    In the image below, we can see that at a certain point, the frequency of the 
    individual cores stays the same, but the number of cores inside a device 
    started to grow. Power consumption per core also stayed the same, but 
    performance still kept increasing!
    
    [INSERT IMAGE]

*** Some physics

    Below we explain why power decreases as you pack more cores together.

    The capacitance C of a core is defined as $C = q*V$ where q is the charge 
    and V is the voltage. Work is defined as $W = V*q$. Therefore, this implies 
    that $W = CV^2$. Since Power p is defined as $P = W*f$ where $f$ is the 
    frequency, we obtain that $P = C*V^2*f$.

    So Power scales linearly in capacitance and frequency, but quadratically in 
    voltage. 

    So given a single core with a fixed frequency and desired ouput, we have 
    some fixed power consumption. 

    However, taking two cores with half the frequency (capacitance doubles) and 
    same output, we find that power goes down!

    So we are able to obtain the same output but with less power. This means 
    we can keep increasing output while optimizing for power. 

    So this is why parallel computing will be the new paradigm.

*** From Shared Memory to Distributed Memory

    As more and more cores began to be packed inside a single CPU, a new problem
    arose: the classic architecture which relied on the front side bus and the 
    northbridge to communicate between CPU and RAM became a major bottleneck
    since all the cores were fighting for the same resource. 

    For this reason, the northbridge was moved inside the CPU and more lanes 
    were introduced that connected to RAM. 

    Finally, to scale even more, multi-socket motherboards were introduced which 
    could use multiple CPU's (a.k.a sockets). The sockets were all connected 
    together through some technology such as Quick Path Interconnect, which 
    effectively made all memory visible to all the sockets.

    This introduced a non-uniform memory (NUMA) access paradigm since each 
    socket had a faster access time to its closest memory bank, but could still 
    access other banks which were farther away, at a higher cost. 

    Lastly, to scale even further, many of these "nodes", a.k.a a multi-socket
    device, were connected together by a very fast network (internet). This is 
    the so called "distributed memory" paradigm, where nodes need to explicitly 
    send messages (message passing) to each other throught the network. 
    To make this paradigm usable, the network needs to be very fast, and since 
    the protocol is less strict than http or https, typically the bandwith of 
    these networks are much higher than standard internet connections with 
    bandwiths of 100 Gbit/s. 

    A last important note is that the shared memory and distributed memory 
    paradigms are a concept at software level and can be completely detached 
    from the physical hardware level. 

    In other words, it is the */way/* we code that changes, and this is 
    independent of the hardware we have. In fact, we can do message passing on 
    a single node machine such as our laptop! In this case, each of our cores 
    will act as if they were a separate process with their own memory, and the 
    programmer will need to explicitly send messages between cores in the 
    code. 

    *How* this happens will then be determined at hardware level. In the 
    case of a multi-node machine, it will be done through the network, while in 
    the case of a single-node machine such as a laptop, it will be done through 
    normal data sharing which will be much faster.

    Viceversa, there exist some libraries which enable one to code in shared 
    memory while on a multi-node machine. Of course, the library will do some
    message passing under the hood, but the programmer */thinks/* in shared 
    memory.

    This is a very useful concept which must be clear - the separation between 
    software level constructs and hardware level requirements.

** OpenMP 

   A CPU can have many threads which are running concurrently.

   The fundamental idea of OpenMP is to enable an interface for managing threads.
   often interleave each other and no pararellization is achieved. 

   OpenMP instead enables the creation of threads which all execute the same block
   of code (called a structured block). This is done within a fork-join framework 
   where the master thread (thread 0), upon finding directives of OpenMP for
   thread creation, "forks" (creates) many threads which execute the block of code
   in a concurrent and parallel fashion. After these threads exit the block of code,
   they are once again joined to the master thread.

   It is important to understand that threads share all the memory before the their
   creation, and additionally have their own private memory!

   Something to keep in mind is that we must be careful with race conditions.

*** Concurrency and Parallelism

    - Concurrency: a property of a system when multiple things *may* be happening 
      at the same time
    - Paralellism: a subset of concurrency. We are taking some of the 
      concurrency and actually executing at the same time.

    So we may have concurrent threads which are not parallel since they are just 
    swapping in and out of execution. 

    On the other hand, parallel threads are executing instructions at the same 
    time.

    The idea is that we need to expose the concurrency of a problem and then map 
    it to processing units so they execute at the same time (paralellism).

    A concurrent application has concurrency built into the problem definition
    In concurrent applications, there is no way to have the problem 
    run without concurrency.

    In a parallel application there are things happening at the same time to make 
    it run faster. In a parallel problem, we don't necessarily *need* to run 
    in parallel. 

*** Description OpenMP

    OpenMP is a set of compiler directives and runtime libraries to be able to 
    run code in parallel.

    At the low level, it assumes shared address space. 

    On top of it, we have an OS that supports a multi-threaded approach.

    The programming layer is all the commands we will describe to enable the 
    parallel execution.


*** Syntax
**** Core Syntax

     OpenMP (OMP) consists of compiler directives of the form
     `#pragma omp [construct] [clause1] [clause2] ...`. Most of the constructs
     of OMP apply to a "structured block"", which is simply a block of code 
     with an entry point at the top and another at the end. 

     We also have some functions and variables that are declared in 
     `#include <omp.h>`.

     @code c
     #include <omp.h>

     int main(){

        //
        // some serial code done by one thread
        //

         // Indicates the start of a parallel region where multiple threads will
         // be created and will execute the block of code inside the structured 
         // block denoted by {}
         #pragma omp parallel        
         {
             // some code block           
         }                           

     }
     @end

     OMP uses the master-fork-join paradigm, i.e. one in which a master thread 
     spawns some threads (a fork) which are joined at the end of a parallel 
     region.

     Each of the threads has a private stack frame, but share the stack of the 
     parent thread. 

     Nested paralell regions are allowed. 

     Note that inside the structured block each thread is executing the code 
     and has a private memory of each variable declared in the block. However, 
     any variables declared before the #pragma omp parallel can be seen by all 
     threads. 

     `omp_set_num_threads(number)`: This function sets the number of threads that OpenMP 
     will *try* to create. It is important to note that it only asks for that number
     of threads, but the OS may give less (perhaps to avoid some crashes or resource 
     overallocation)

     `omp_get_num_threads()`: returns an int. Gives the number of actual threads running.
     Useful to check how many threads I actually have.

     `omp_get_thread_num()`: returns an int. Gives the id of the current thread that
     calls this function, i.e. if I have 5 threads and the third thread calls this 
     function, it will return 2 (note we start counting from 0)

** False Sharing

   False sharing is an important problem we must always keep in mind to avoid
   serious performance hits. Basically, it occurs when we have a shared variable 
   among all threads. Suppose all threads want to read this variable, then, a copy 
   of the variable will be pulled in the cache of the core belonging to a specific
   thread. Suppose that all threads then want to write to this variable and change it. 
   They will each modify their local version, which in turn will activate a cache coherency
   protocol which ensures that the local copy of the variable in memory is always coherent.
   This means however, that we will have a lot moving back and forth of the variable
   among caches as it is updated. Thus performance will be terrible.

** Synchronization

   two types:
   - barrier
   - exclusion

*** barrier

    OMP provides functionality to ensure that all threads are synchronized at a barrier.
    This can be achieved by the directive:
    `#pragma omp barrier`

    Upon finding this directive, all threads will wait until all the threads have called
    the barrier and then keep going to the next block of code.

    // insert example

    Subtle point: By default, the barrier is inserted automatically after any worksharing
    construct, which we will talk about later. However, it is important to keep in mind
    that while the barrier is useful to make sure that all threads are synchronized at 
    some point, if one doesn't truly need the barrier, this can slow down performance.

*** exclusion

    OMP provides a few constructs for exclusion, and they vary in their granularity of 
    low level control that we can achieve.

    1. '#pragma omp critical'  : This directive ensures that only one thread at a time
    is executing the code found in the subsequent structured block. Note that all other
    threads will be waiting before this directive to be able to execute the code block.
    Therefore, if not needed, this could slow down performance. 

    2. '#pragma omp atomic' : This directive is similar to atomic, but is slightly more 
    low level in the sense that for simple operations, it can take advantage of some 
    hardware functionality for switching between threads. If this functionality is not 
    available, it will be identical to '#pragma omp critical'.

    3. Locks: Locks are the most low level directives that are offered by OMP. In a 
    few words, locks are variables of a type declared in <OMP.h> and they enable rich
    functionality:
    - omp_set_lock() : the thread that calls this function has the lock and can 
    execute the code requiring that lock 
    - omp_get_lock() : the thread that calls this function tries to obtain the 
    lock, and if it fails, it will wait until the lock is available.
    - omp_test_locl() : the thread will test whether the lock is free. If it isn't 
    it will keep on going to do other stuff instead of waiting.

    EXAMPLE 


** Worksharing constructs:

   OMP provides some worksharing construct. As an example, consider:

   @code c
   #pragma omp parallel{
   #pragma omp for

    }
   @end

   sections


** Reductions


* References 
  - An amazing {https://www.youtube.com/playlist?list=PLLbPZJxtMs4ZHSamRRYCtvowRS0qIwC-I}[tutorial] by Tim Mattson from Intel.
    He was one of the original developers of OpenMP and explains all the 
    concepts with amazing clarity.
  -- The repository of the {https://github.com/tgmattso/OpenMP_intro_tutorial}[course] with all the code and exercises he discusses
  - Luca Tornatore's slides on the OpenMP portion of the HPC course for UniTS. 
  -- {https://github.com/Foundations-of-HPC/Foundations_of_HPC_2020}[HPC 2020]
  -- {https://github.com/Foundations-of-HPC/Foundations_of_HPC_2021/tree/main/OpenMP}[HPC 2021]
  -- {https://github.com/Foundations-of-HPC/Foundations_of_HPC_2022/tree/main/Basic/OpenMP}[HPC 2022]
  - https://github.com/radix-io/hands-on/tree/main/mpi2tutorial/examples/life

