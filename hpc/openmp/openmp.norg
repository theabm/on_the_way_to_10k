* OpenMP

** Moore's law

   Goordon Moore came up with an empirical law that stated that every 2 years, 
   the number of transistors that on a device would double. 

   As a consequence, manufactures would pack more transistors into a single 
   core and increase performance.

   Many years later, Moore's law is still working. Over the years we see that 
   performance keeps increasing.

   Because of this, people were trained to expect that performance would come 
   automatically from hardware at no cost. This was known as the "Free Ride" 
   era since every two years the performance of any application would double 
   for "free" as long as one would get the latest processor.

   As a consequence, many programmers were not taught to explicitly think about 
   performance.

   However, something happened that stopped this trend. 

*** The power wall problem

    If we plot the power vs. performance, it can be seen that as performance 
    increases, the power consumption increases almost quadratically. 

    This meant that the trend of packing more transistors in a single core to 
    increase performance would be unsustainable in the long term, which meant 
    the end of the Free Ride era.

    This caused manufactures to try to optimize for power as well as 
    performance. They began to manufacture cores with less transistors and 
    therefore less frequency (i.e. a simpler architecture). However, they began
    to put more cores into a single device (CPU)!

    This was the start of the multicore era and the shared memory paradigm where 
    cores would need to share the RAM (and other resources) among themselves 
    efficiently in order to obtain increased performance.

*** Some physics

    Below we explain why power decreases as you pack more cores together.

    The capacitance C of a core is defined as $C = q*V$ where q is the charge 
    and V is the voltage. Work is defined as $W = V*q$. Therefore, this implies 
    that $W = CV^2$. Since power p is defined as $P = W*f$ where $f$ is the 
    frequency, we obtain that $P = C*V^2*f$.

    So power scales linearly in capacitance and frequency, but quadratically in 
    voltage. 

    So given a single core with a fixed frequency and desired ouput, we have 
    some fixed power consumption. 

    However, taking two cores with half the frequency (capacitance doubles) and 
    same output, we find that power goes down!

    So we are able to obtain the same output but with less power. This means 
    we can keep increasing output while optimizing for power. 

    So this is why parallel computing will be the new paradigm.

*** From Shared Memory to Distributed Memory

    As more and more cores began to be packed inside a single CPU, a new problem
    arose: the classic architecture which relied on the front side bus and the 
    northbridge to communicate between CPU and RAM became a major bottleneck
    since all the cores were fighting for the same resource. 

    For this reason, the northbridge was moved inside the CPU and more lanes 
    were introduced that connected to RAM. 

    Finally, to scale even more, multi-socket motherboards were introduced which 
    could use multiple CPU's (a.k.a sockets). The sockets were all connected 
    together through some technology such as Quick Path Interconnect, which 
    effectively made all memory visible to all the sockets.

    This introduced a non-uniform memory (NUMA) access paradigm since each 
    socket had a faster access time to its closest memory bank, but could still 
    access other banks which were farther away, at a higher cost. 

    Lastly, to scale even further, many of these "nodes", a.k.a a multi-socket
    device, were connected together by a very fast network (internet). This is 
    the so called "distributed memory" paradigm, where nodes need to explicitly 
    send messages (message passing) to each other throught the network. 
    To make this paradigm usable, the network needs to be very fast, and since 
    the protocol is less strict than http or https, typically the bandwith of 
    these networks are much higher than standard internet connections with 
    bandwiths of 100 Gbit/s. 

    A last important note is that the shared memory and distributed memory 
    paradigms are a concept at software level and can be completely detached 
    from the physical hardware level. 

    In other words, it is the */way/* we code that changes, and this is 
    independent of the hardware we have. In fact, we can do message passing on 
    a single node machine such as our laptop! In this case, each of our cores 
    will act as if they were a separate process with their own memory, and the 
    programmer will need to explicitly send messages between cores in the 
    code. 

    *How* this happens will then be determined at hardware level. In the 
    case of a multi-node machine, it will be done through the network, while in 
    the case of a single-node machine such as a laptop, it will be done through 
    normal data sharing which will be much faster.

    Viceversa, there exist some libraries which enable one to code in shared 
    memory while on a multi-node machine. Of course, the library will do some
    message passing under the hood, but the programmer */thinks/* in shared 
    memory.

    This is a very useful concept which must be clear - the separation between 
    software level constructs and hardware level requirements.

** OpenMP 

   A CPU can have many threads which are running concurrently.

   The fundamental idea of OpenMP is to enable an interface for managing threads
   which otherwise interleave each other.

   OpenMP instead enables the creation of threads which all execute the same 
   block of code (called a structured block). This is done within a fork-join 
   framework where the master thread (thread 0), upon finding directives of 
   OpenMP for thread creation, "forks" (creates) many threads which execute the 
   block of code in a concurrent and parallel fashion. After these threads exit 
   the block of code, they are once again joined to the master thread.

   It is important to understand that threads share all the memory before the 
   their creation, and additionally have their own private memory!

   Something to keep in mind is that we must be careful with race conditions.

*** Concurrency and Parallelism

    - Concurrency: a property of a system when multiple things *may* be happening 
      at the same time.
    - Paralellism: a subset of concurrency. We are taking some of the 
      concurrency and actually executing at the same time.

    *insert conc_vs_par.jpg*

    So we may have concurrent threads which are not parallel since they are just 
    swapping in and out of execution. 

    On the other hand, parallel threads are executing instructions at the same 
    time.

    The idea is that we need to expose the concurrency of a problem and then map 
    it to processing units so they execute at the same time (paralellism).

*** Description OpenMP

    OpenMP is a set of compiler directives and runtime libraries to be able to 
    run code in parallel.

    It makes it as easy as possible to write a parallel program.

    At the low level, it assumes shared address space. 
    On top of it, we have an OS that supports a multi-threaded approach.

    Finally, at the top is the programming layer, which is all the commands, 
    environment variables, and OMP libraries that we will describe to enable 
    parallel execution.

*** Core Syntax

     OpenMP (OMP) consists of compiler directives of the form

     @code c
     // this library includes functions and variables that we need to 
     // run openmp
     #include <omp.h>
     #pragma omp construct [clause1] [clause2] ...

     @end
     
     Most of the constructs of OMP apply to a "structured block", which is 
     simply a block of code with an entry point at the top and another at the 
     end. 

     Conceptually, our code will look something like this:

     @code c
     #include <omp.h>

     int main(){

        //
        // some serial code done by one thread
        //

        // Indicates the start of a parallel region where multiple threads will
        // be created and will execute the structured block of code inside {}

        #pragma omp parallel        
        {
             // some code block (a.k.a structured block).
             // multiple threads will execute this block of code 
             // in parallel.
        }                           

     }
     @end

     The #pragma omp parallel directive tells the compiler to spawn some threads 
     which will execute the code block. The number of threads depends on the 
     OS, however, we will talk about this in more detail later.

     OMP uses the master-fork-join paradigm, i.e. one in which a *master* thread 
     spawns some threads (a *fork*) which are *joined* at the end of a parallel 
     region.

     Each of the threads has a private stack frame, but share the stack of the 
     parent thread (also called the master thread). This means all of the 
     variables declared inside the code block are private to each thread, while 
     variables declared *before* the code block are visible by all the threads.
     This has important consequences in how we handle variables as we will 
     see later.
        
     Below is a very simple omp program.

     #tangle hello.v1.c
     @code c 
     #include <omp.h>
     #include <stdio.h>
     int main()
     {

         printf("I am the master thread and I will print only once!\n");
         #pragma omp parallel
         {
             printf("I am a lowly thread among many, hello!\n");
         }
     }
     @end

     We compile this program as "gcc -fopenmp hello.v1.c -o hello.v1", 
     to obtain the executable. Note that to use openmp, we have to compile 
     with the flag *-fopenmp*. 

     If we run the executable, we will see that the phrase hello is printed 
     multiple times in standard output (once by each thread) while the first 
     print statement is executed only once.

*** Some initial run-time commands

    Now we will describe some useful run-time commands for beginners.

    - `omp_set_num_threads(int number)` -> void : This function sets the number
    of threads that OpenMP will *try* to create. It is important to note that 
    it only asks for that number of threads, but the OS may give less to avoid 
    some crashes or resource overallocation.

    - `omp_get_num_threads()` -> int : Gives the total threads that have been 
    spawned. Useful to check how many threads I actually have.

    - `omp_get_thread_num()` -> int : Gives the id of the current thread that
    calls this function, i.e. if I have 5 threads and the third thread calls 
    this function, it will return 2 (note that we start counting from 0)

    - `omp_get_wtime()` -> double : return the number of seconds with respect 
    to some time in the past.

*** Assumed hardware/software

    Today, all machines are NUMA (non-uniform memory access) machines. Even a 
    single processor level, each core has its own L1 cache, therefore memory 
    access times are not uniform. 

    As we know a program has a stack, a heap, a text, and a data region. Modern 
    OS's decompose further into threads by fragmenting the stack. Each thread has 
    its private stack, which makes it cheap to switch between threads based on 
    context (this is handled by the OS). However, all threads share the heap, 
    text, and data regions. 

    So in summary, all the threads have their private stack, but share the heap, 
    which belongs to the process that is calling the program execution.

    *include threads_vs_process.jpg*

    Note that we can have more threads than cores! Nothing stops us from being 
    in this scenario. Furthermore, when a program is running, we must think of 
    all the  possible ways that the threads can be interleaved to give a correct 
    execution, since these may actually happen. This is why, in the program 
    below, the order of the output is random.

     #tangle hello.v2.c
     @code c 
     #include <omp.h>
     #include <stdio.h>
     int main()
     {

         #pragma omp parallel
         {
             int my_id = omp_get_thread_num();
             int num_threads = omp_get_num_threads();

             printf("Hi from thread %d of %d\n", my_id, num_threads);
         }
     }
     @end

    The order is random since the threads can interleave in any possible order.
    This concept will be important later!

    Furthermore, it is important to understand that threads communicate with 
    each other through shared variables.

*** Race Conditions

    Sometimes, threads share data in an unsafe way, i.e. a thread writes in
    a location (a variable) that another thread is trying to read. 

    This leads to a so called "race condition" because the outcome of the 
    program depends on what happened at that particular runtime (did the thread 
    write before the other one managed to read? or viceversa?).

    Race conditions are the biggest problem in parallel programming.

    How do we prevent race conditions? By controlling and organizing access to 
    shared variables. 
    We can achieve this through {# Synchronization}[synchronization]. 

    However, {# Synchronization}[synchronization] is very expensive and we must try to minimize it 
    as much as possible. We do this by being careful with how we organize our 
    code and data environment.

*** How to create threads in OpenMP

    As mentioned above, OMP uses a fork-join model. At the beginning, our program 
    has a single thread which we call the master thread. 
    Then it encounters a portion of execution where more threads can help out.  

    The master thread forks threads at that point. The master thread continues 
    and it has ID 0 while the other threads all have increasing ID's. This is 
    called a collection of threads. 

    When they all finish their work, they join back together again and we are 
    left with one thread. 

    This process can be repeated as much as needed, and we can even nest threads.

    This is done through the #pragma omp parallel construct that we saw before.
    And this is the *only* way we can create multiple threads using OMP.

    Take a look at the following code:

    @code c 
    int main(){
        double A[1000];
        // request 4 threads (OS will try to satisfy, not guaranteed)
        omp_set_num_threads(4);
        #pragma omp parallel
        {
            // each thread will run this block of code
            int ID = omp_get_thread_num();
            foo(ID,A);
        }
    }
    @end

    If we allocate data outside the #pragma omp parallel, it is in the heap, and 
    it is shared among all threads. 
    If I allocate the data inside the #pragma omp parallel, it is allocated in 
    the threads stack. So it is private.

    So in this case, all threads see the same copy of array A, and they all 
    call the function foo() with the same copy of A, but with their *own* 
    ID.

    If we were to print out the address of ID inside the code block, we would 
    see that they are all different. On the other hand, if we print the address 
    of A\[0\] from inside the code block, they all print the same address.

    But what does the compiler do? It takes the structured block, and packages 
    it into a function called a "thunk". Then it sets up the p-threads environment 
    and it launches each p-thread using a copy of the thunk.

    The advantage of using OMP is that we don't have to deal with the low level 
    interface of p-threads.

*** \[Exercise\] Hello World of Parallel Computing

    Below is a serial program that calculates pi.

    Using only the constructs seen so far, how would you parallelize it?

    #tangle serialpi.c
    @code c 
    #include <stdio.h>
    #include <omp.h>
    static long num_steps = 100000000;
    double step;
    int main()
    {
        int i;
        double x, pi, sum = 0.0;
        double t0 = omp_get_wtime();

        step = 1.0/(double)num_steps;
        for (i=0; i<num_steps;i++){
            x = (i+0.5)*step;
            sum += 4.0/(1.0+x*x);
        }
        pi = step * sum;
        double t1 = omp_get_wtime();

        printf("pi is %f\n", pi);
        printf("execution time is %f s \n", t1-t0);
    }
    @end

    The first thing we must try to do is to expose some concurrency in the 
    problem. 

    A first simple idea, is to divide the work of the for loop among more 
    threads. Can we do this? 

    #tangle parallelpi.wrong.c
    @code c 
    #include <stdio.h>
    #include <omp.h>

    #define NUM_THREADS 20

    static long num_steps = 100000000;
    double step;
    int main()
    {
        double pi = 0.0;
        int nthreads;
        double t0 = omp_get_wtime();
        double partial_sum[NUM_THREADS];

        step = 1.0/(double)num_steps;

        // we *request* these threads, but the OS can give us less!
        omp_set_num_threads(NUM_THREADS);

        #pragma omp parallel
        {
            double x = 0.0;
            int i;

            // we check since it is not guaranteed that 
            // we get 10 threads
            int num_threads = omp_get_num_threads();
            // I get my id
            int my_id = omp_get_thread_num();

            // the master thread updates the shared variable 
            if(my_id==0) nthreads = num_threads;

            for (i=my_id, partial_sum[my_id] = 0.0; i<num_steps;i+=num_threads){
            x = (i+0.5)*step;
            partial_sum[my_id] += 4.0/(1.0+x*x);
            }
            }
        for(int i = 0; i<nthreads;++i){
        pi+=partial_sum[i]*step;
        }
        double t1 = omp_get_wtime();

        printf("pi is %f\n", pi);
        printf("execution time is %f s \n", t1-t0);
    }
    @end
    
    First of all, if you came up with the code above on your own, 
    congratulations! You are officially a parallel programmer. 
    If you didn't, that's okay too. Let's see if I can explain what's going on!

    The main idea is that each thread will process an iteration of the 
    foor loop, in a round robin fashion (also called "cyclic distribution"). 
    But let's look at the variables we were using in more detail.

    Variable x is a function of the loop index and, as such, it can be 
    calculated independently from other iterations. This is good, because each 
    thread can do its own thing without bothering anyone. 

    However, we can't leave it as it was before! Why? 

    Because "variables declared outside of the parallel construct are *shared* 
    among threads". So basically, while each thread is doing its own thing, as 
    soon as its done calculating its expression for x, it will race against 
    other threads for who gets to write at x first.

    In other words, we have a race condition (remember those? yuck.) 

    To fix this, we can simply declare x inside the parallel construct, making 
    it *private*. Now, each thread can do its own thing and write on its own 
    variable x.  

    The same idea applies to the variable sum. However, in this case, we can't 
    solve it in the same way because each thread has to somehow *communicate* its 
    partial sum to the overall result.

    Also, as soon as we exit the parallel construct, the stack of each thread 
    is deallocated, i.e. the variables declared inside are lost. So we can't 
    use a local sum variable, as we did for x, because we have to somehow get 
    this information out of the parallel regions.

    Therefore, the partial sum of each thread *must* somehow be shared! How 
    can we achieve this? Through an array.

    We create a partial sum array, which is as long as the maximum number of 
    threads we are hoping to get. Each thread then writes in its own "slot" 
    and at the end, we sum all the partial values in the array to calculate pi.

    So why is this code wrong? As we can see, if we run the code, we obtain a 
    correct answer for pi, however, it the performance is considerably worse 
    than the serial case!

    In the next sections, we will dive into the pitfalls of this code and learn 
    how to fix it. 

** The Problem of False Sharing

   One of the reasons our code performs terribly is *False Sharing*. 

   It is an important problem we must always keep in mind to avoid
   serious performance hits. 

   Basically, it occurs when we have a shared variable among all threads. 

   Suppose that we have as many threads as cores, and that all threads want to 
   read this variable. Then, a copy of the variable will be pulled into the 
   cache of each core. 

   Now, imagine that all threads want to write to this variable and change it. 
   They will each modify their local copy in the cache, which in turn will 
   activate a cache coherency protocol. This protocol ensures that the local 
   copy in cache is coherent with the copy in memory.

   For example, if we use the MESI protocol, the local variable will be marked 
   with M (modified) while all the other copies, along with the one in main 
   memory will be marked with I (invalid).

   As a consequence, the local copy in cache is written in main memory and then 
   all other threads *flush* their local cache copy, and update their copy 
   of the variable. 

   This process is expensive, and requires a lot of memory movement, which is 
   one of the biggest performance killers in parallel applications since it 
   is an unwanted synchronization. (remember synchronization?)

   In our example above, the partial sum array is most likely sitting 
   in the same cache line. So, whenever a thread updates its "slot", the 
   coherency protocol is triggered, and all the other threads must pause what 
   they are doing, flush the *whole* array, update it, and then continue on. 
   And this may happen many many times, killing performance.

*** False Sharing Solution (1)

    A possible solution to our problem is to pad our array, so that all the 
    partial sum slots reside in different cache lines.

    So, if we assume a 64 Byte L1 cache line, then it will fit 8 doubles values. 
    Therefore, our partial sum array will be declared as  
    @code c 
    #define PAD_SIZE 8

    //...

    double partial_sum [NUM_THREADS][PAD_SIZE]

    // ... 

    partial_sum[my_id][0] += 4.0/(1.0+x*x);
    @end

    So now, the slot of each thread is in its own cache line, and they can 
    safely modify their own slot without triggering the coherency protocol.

    Of course, this is an ugly solution because it requires knowing the size 
    of the cache line. So it kills portability!

    We will look at better solutions along this tutorial.

** An issue of synchronization

   Synchronization is a delicate concept, and it can get very technical. 
   Fortunately, OMP keeps it simple.

   There are two main types of synchronization:

   - Barrier: Each thread waits at the barrier until all threads arrive.
   - Mutual Exclusion: Define a block of code that only one thread at a time can 
     execute.

    How do we achieve this in OMP?

*** Barrier construct

    The simplest synchronization construct is the barrier.
    @code c
    //...
    #pragma omp parallel 
    {
        int id = omp_get_thread_num();
        // some big calculation that depends on the thread id
        A[id] = big_calc1(id);

        // Barrier. This ensures *all* threads have calculated A before going on 
        // to the next part.
        #pragma omp barrier
        // We need a barrier because, to calculate B, we need all A to be 
        // completed. 
        // So we cannot allow one thread to finish and try calculating B when 
        // other threads are still working on A!
        B[id] = big_calc2(id, A);
    }

    @end

    We introduce now a subtle point. By default, a barrier is inserted 
    automatically after all worksharing constructs (which we will see in a bit).

    However, it is important to keep in mind that while the barrier is useful 
    to make sure that all threads are synchronized at some point, if one 
    doesn't truly need a barrier, it can slow down performance.

    So make sure your barriers are actually needed.

*** Mutual Exclusion

    OMP provides a few constructs for exclusion, and they vary in their 
    granularity of low level control that we can achieve.

**** Critical 

     The easiest construct to achieve mutual exclusion is the critical construct

     The critical construct creates a *critical section*, which ensures that only
     one thread at a time is executing the code found in the subsequent 
     structured block. 

     Note that all other threads will be waiting before this directive for their 
     turn to execute the code block. In other words, the critical construct 
     /serializes/ our code for some time. Therefore, if not needed, this could 
     slow down performance. 

     @code c
     #pragma omp parallel 
     {
         // ... 
         #pragma omp critical 
         {
             // critical section
         }

     }
     @end

     A pro tip: Since a critical directive serializes our code, make sure that 
     the work that has to be done in the critical section is not too big! 
     Otherwise performance will take a serious hit.

**** Atomic

     The atomic directive is very nuanced and subtle, and will be discussed in
     more detail later. 

     For now, lets keep it simple.

     There are certain constructs supported in hardware that allow for quick 
     updates of values of values in memory. The atomic construct tries to use
     these hardware constructs if they are available. Otherwise, it behaves 
     like a critical construct.

     @code c
     #pragma omp parallel 
     {
         // ... 
         #pragma omp atomic 
         {
             // critical section
         }

     }
     @end

*** \[Exercise\] Parallel Pi

    It's time to fix our program!

    Using all the constructs we have seen so far, how would you fix our previous 
    program to avoid false sharing and become blazingly fast?

    Here is a possible solution using the atomic construct.

    #tangle parallelpi.atomic.c
    @code c 
    #include <stdio.h>
    #include <omp.h>

    static long int num_steps = 10000000;
    double step;

    #define NUM_THREADS 100

    int main(){
        double start = omp_get_wtime();
        double pi, sum = 0.0;

        step = 1.0/(double) num_steps;

        omp_set_num_threads(NUM_THREADS);

        #pragma omp parallel
        {
            int num_threads = omp_get_num_threads();
            int id = omp_get_thread_num();

            double x, partial_sum = 0.0;

            for(int j = id; j < num_steps; j+=num_threads){
                x = (j + 0.5)*step;
                partial_sum += 4.0/(1.0+x*x);
            }

            #pragma omp atomic
                sum+=partial_sum;

        }
        pi = sum*step;
        
        double end = omp_get_wtime();

        printf("Pi is: %f\t Wtime is: %f\n", pi, end-start);
    }
    @end
    
    As we can see, our code is very similar to before. 

    However, now we have a variable "partial sum" which is private for each 
    thread. Each thread accumulates its partial sum in this variable, and when 
    it's done, it updates a global variable "sum". However, to avoid race  
    conditions, we must use synchronization. 

    This is achieved with the atomic construct (we could also have used the 
    critical construct).

    The performance *may* be similar to the padded solution. However, this is 
    much more portable!

    - (!) Warning: Be careful not to make the mistake of putting the critical 
    construct inside the for loop (to calculate pi directly). If you do this, 
    your code becomes basically serialized since each thread spends very 
    little time calculating x. So you lose performance. This is a typical 
    mistake when approaching synchronization constructs for the first time.

**** Locks - (Advanced!)

     Locks are the most low level directives that are offered by OMP. In a 
     few words, locks are variables of a type declared in <OMP.h> and they enable 
     rich functionality:

     In a few words, if a thread holds a lock, it can do what it wants. 
     If a thread tries to grab a lock, and it is held by someone else, it must 
     wait until the lock is freed.

     However, since the lock is a variable, we can obtain more complex, flexible 
     behavior compared to using critical or atomic.

     - omp_init_lock() : initialize the lock variable.

     - omp_set_lock() : the thread that calls this function obtains the lock if 
     the lock is available. If the lock is unavailable, it will wait until 
     it becomes available (inefficient!). The thread that has the lock can execute the code 
     requiring that lock 

     - omp_unset_lock() : frees the lock and makes it available for grabs by 
     other threads. 

     - omp_destroy_lock() : destroys the lock so it is no longer available. 
     Similar to freeing allocated memory in C.

     - omp_test_lock() : the thread will test whether the lock is free. If it 
     is, it will set the lock and retun. If it isn't it will return and the 
     thread can keep on going to do other stuff instead of waiting. 
     This is in contrast to omp_set_lock() where a thread has to wait if the 
     lock is not available. 

     EXAMPLE IN VIDEO 14 - Intro to OPENMP 11 PART 2 ABOUT HISTOGRAMS ----------------------------------------------------------

** OMP Worksharing constructs:

   Worksharing is an extremely important feature of OMP.

*** The loop worksharing construct:

    One of the most important and common worksharing constructs is loops. 

    Imagine we want to divide the work of a for loop in the same way we did 
    above. However, we want to avoid having to go through the hassle of manually 
    splitting up the iterations according to thread id.

    Ideally, what we want is to let the compiler figure things out.

    OMP provides *just* the right construct for that!

    @code c 
    #pragma omp parallel
    {
        #pragma omp for
        for (i = 0; i<N; ++i)
        {
            // work

        }
    }
    @end

    We can also specify *how* work is divided among threads. This is done 
    through the `schedule` clause.

**** Scheduling

     - `schedule(static [,chunk])` : Deal out blocks of iterations of size "chunk" 
     to each thread. Note that the chunk parameter is optional, and can be left 
     to the compiler. 

     All of this is done at compile time. Note that if we have 
     less threads than blocks, for example, a loop of 100 iterations and chunk 
     20, i.e. 5 blocks, and 2 threads, the compiler will distribute work in a 
     round robin fashion.

     - `schedule(dynamic [,chunk])` : Each thread grabs "chunk" iterations off 
     a queue until all iterations have been handled. Again, the chunk parameter 
     is optional. 

     This is done at runtime. This is advantageous if we have very different 
     workloads between different runs or if we don't know ahead of time the 
     workload! (For example, maybe we are always changing the data set we are 
     working with). 

     Of course, the price of this flexibility is some additional runtime 
     functions which are added to our code. So if we *know* the workload ahead 
     of time, a static schedule will be faster (simply because it doesnt need 
     to account for this flexibility).

     - `schedule(guided [,chunk])` : Threads dynamically grab blocks of 
     iterations. The size of the block starts large and shrinks down to size 
     "chunk" as the calculation proceeds.

     This construct is not very popular.

     - `schedule(runtime)` : Schedule and chunk size taken from the OMP_SCHEDULE
     environment variable (or a runtime library). 
     This is used a lot when we don't know which schedule is best and we want to 
     try different options without having to recompile our code each time.
     This can be combined with 
     -- `omp_set_schedule("static"|"dynamic"|"auto")` : sets the schedule at 
        runtime.
     -- `omp_get_schedule()` : get the schedule type.

     - `schedule(auto)` : A relatively new option. The schedule is left up to 
     the runtime to choose (does not necessarily have to be any of the above 
     options, and the compiler may do some complicated scheduling).
     This option gives the compiler the most flexibility in choosing the 
     schedule.

     Pro tip: We have listed all the schedules for completeness. In the 
     beginning, only use static or dynamic.

     Pro tip 2: Use static when we know the workload ahead of time. Otherwise, 
     use dynamic to ensure a good load balancing among threads (i.e. they do 
     equal work).

**** A combined notation

     Since one of the most popular uses of OMP is to parallelize 
     loops, it is very common to find code snippets like the one above where 
     the *only* parallelization is a for loop. 

     To make notation shorter, OMP supports a combined notation:

     @code c 
     #pragma omp parallel for
     for (i = 0; i<N; ++i)
     {
        // work

     }

     @end

     This is syntactic sugar for the previous code snippet.

** Reductions

   One of the most common uses cases of parallel progamming is to speed up 
   an operation over a for loop, such as finding the sum, the max, the min,
   the product, etc.

   These type of operations are so important, that they are given a specific 
   name: Reductions (because they *reduce* the array to a single value we care 
   about).

   OMP supports reductions out of the box, as clauses that are added at the end 
   of the for loop construct.

   The syntax is:

   `reduction(operator:list of variables)`

   Lets dive into more detail. 

   This clauses tells the compiler to make a local copy of each of the 
   variables in the list and initialize them depending on the operation. 

   - For sum : initialized to 0
   - For product : intialized to 1
   - etc.

   Then, updates occur on the local copy. 
   At the end of the entire process, local copies are reduced into a single 
   value and combined with the original global value.

   This is *all* done under the hood by the compiler! So nice!

   Below is a quick example:

   @code c 
   double avg = 0.0, A[SIZE]; int i;
   #pragma omp parallel for reduction (+:avg)
   for(i = 0; i<SIZE; ++i){
       avg += A[i];
   }
   avg /= SIZE;
   @end

   Note that by default, the loop index i is made local to each thread under 
   the hood, so we don't need to worry about race conditions.

   However, for clarity, we could declare i locally (which is what I prefer to 
   do).

*** \[Exercise\] Parallel Pi - Round 3

    This one's pretty easy. 

    Try to rewrite the parallel pi using the worksharing constructs we have 
    seen so far!

    As usual, try to come up with your own solution before looking at the 
    answer.

    #tangle parallelpi.reductions.c
    @code c
    #include <stdio.h>
    #include <omp.h>

    static long int num_steps = 100000000;

    double step;
    #define NUM_THREADS 100

    int main(){
        double x, pi, sum = 0.0;

        step = 1.0/(double) num_steps;
        double start = omp_get_wtime();

        omp_set_num_threads(NUM_THREADS);
        #pragma omp parallel for reduction (+:sum, x)
        for (int i=0; i<num_steps; i++){
            x = (i+0.5)*step;
            sum += 4.0/(1.0+x*x);
        }
        pi = step*sum;
        double end = omp_get_wtime();

        printf("Pi is: %f\t Wtime is: %f\n", pi, end-start);
    }
    @end

    Boom! Now it looks much cleaner doesn't it?

    Notice that the variable x has to be local to each thread. We can do this in 
    two ways: 
    - Declaring it inside the #pragma omp parallel region. To do this, we cannot 
      use the compact notation of the for loop.
    - Using the compact notation, put it in the list of variables. However, this 
      will insert some useless code to recombine the local copies to the global 
      variable. 

    There are more elegant ways which we will see later to deal with this.

** Working with OpenMP

   Now we are going to talk about a collection of constructs that require a good 
   overall grasp of OMP.

*** Barrier - Revisited

    Let's talk about where barriers are implied and where they are explicit.

    At the end of a `#pragma omp for` there is an implicit barrier. This was a 
    design choice by OMP designers to increase the safety of the code, since 
    most of the time, after a for loop, you will use that result immediately.

    This holds true for any worksharing construct.

    To override this default behavior, we can use the `nowait` clause. 

    @code c
    #pragma omp parallel 
    {

        #pragma omp for
        for(i = 0; i<N; ++i){
            // big calculations
        }// there is an implicit barrier here. 
        // at the end of this, I am sure that all threads have finished 
        // doing the calculations

        #pragma omp for nowait
        for(i = 0; i<N; ++i){
            // other big calculations
        }

        // ... other calculations which dont depend on results of previous for loop

    }
    @end

    As we can see, the last calculations we make are independent of the for 
    loop before them.
    So we don't need an implicit barrier! And remember, synchronization 
    constructs are performance killers. 

    So by overriding the default behavior, we can gain some performance.

    A word of advice: use this only when you are *_absolutely_* sure of what you 
    are doing.

    Note: there is also an implied barrier at the end of a parallel region which 
    we can't turn off - it would't even make sense to do so.
*** Master and Single

    1. `#pragma omp master` : When I want the master thread only to do something.

    --- ( ) *no* synchronization implied at the end of this construct,
        so all threads keep on going to the next portion of code.

    2. `#pragma omp single` : When I want a *single* thread to do something, and I 
      don't care if it's the master thread. With this construct, the first 
      thread to arrive to the structured code block will execute it.

    --- (x) *implied* synchronization barrier at the end of this (worksharing) 
            construct. So all threads wait at the end of the structured block 
            until the single thread is done.
            Of course, we can put a `nowait` at the end of this clause.

*** Sections

    Gives us a way to create sections of code and each thread does a certain 
    section.

    This is more easily illustrated with an example.

    @code c
    #pragma omp parallel
    {
        #pragma omp sections
        {
            #pragma omp section
            x_calculation();
            #pragma omp section
            y_calculation();
            #pragma omp section
            z_calculation();
        }
    }
    @end
    
    As we can see, we first declare a region which will contain *sections*, and
    then we define the single *section*, one by one.

    In this case, one thread will execute the first section, another the second, 
    and another the third.


** Additional Runtime libraries 



* References 
  - An amazing {https://www.youtube.com/playlist?list=PLLbPZJxtMs4ZHSamRRYCtvowRS0qIwC-I}[tutorial] by Tim Mattson from Intel.
    He was one of the original developers of OpenMP and explains all the 
    concepts with amazing clarity.
  -- The repository of the {https://github.com/tgmattso/OpenMP_intro_tutorial}[course] with all the code and exercises he discusses
  - Luca Tornatore's slides on the OpenMP portion of the HPC course for UniTS. 
  -- {https://github.com/Foundations-of-HPC/Foundations_of_HPC_2020}[HPC 2020]
  -- {https://github.com/Foundations-of-HPC/Foundations_of_HPC_2021/tree/main/OpenMP}[HPC 2021]
  -- {https://github.com/Foundations-of-HPC/Foundations_of_HPC_2022/tree/main/Basic/OpenMP}[HPC 2022]
  - https://github.com/radix-io/hands-on/tree/main/mpi2tutorial/examples/life

