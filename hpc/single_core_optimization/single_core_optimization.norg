* Memory Alignment

  Memory alignment refers to the need to properly place data structures in 
  memory so that the processor may read it efficiently. 

  The first important concept to understand, is that processors do not read from 
  memory in byte sized chunks (as we are accustomed to think about) but rather 
  in multi-byte chunks (depending on your architecture).

  This is referred to as "memory access granularity". In a 32-bit architecture, 
  a processor will read memory in 4 byte chunks (per memory access),
  while in a 64-bit architecture, it will do so in 8 byte chunks.

  Since memory access has an overhead, it is important to reduce it as much as 
  possible to maximize performance (if we care about speed).

  This consideration forces us to think about memory alignment. 

  Suppose we wish are in a 64-bit architecture, and we store a floating point 
  number starting at address 0.

  Since floating point numbers take up 8 bytes, addresses 0-7 will be filled, 
  and to read this number, the processor will need a single memory access.

  However, if we were to store the data beginning at adress 1, the processor 
  will still read a block of 8 bytest starting from adress 0, and then need to 
  read another 8 bytes fromm adress 8 in order to obtain the full representation 
  of the floating point.

  So this requires two memory accesses. In this case, the variable is unaligned.

  The jargon "aligned to X bytes" simply means that we store the value at the 
  appropriate address which is divisible by X. 
  To put it more clearly: An address aligned at 7 bytes means that the address 
  is a multiple of 7. 
  Instead, when we talk about "variable Y aligned at X bytes", we 
  simply use the same concept with variable Y's address.

  So, ideally, we want all our data to be aligned to our systems memory access 
  granularity. 
  In fact, the compiler does this for us automatically when we create new 
  variables, by assigning addresses that are aligned with the data type. 
  This is the reason why, if not careful, a custom data type may take more 
  memory than we expect, since the compiler does some padding to ensure 
  alignment.

* Memory Allocation
  Allocating memory always has an overhead (table maintained, and some other things)
  EXPAND
  - Memory allocation

* Optimization
  - 04 Optimization Basic Steps

** First Steps
   The order of importance:
   - correcteness
   - data model
   - algorithm you choose

   Only after all of this, can we think about optimizing code. 
   You may have beautiful code, but if you're ussing  the wrong 
   algorithm, it will always be worse than a less optimmized code which 
   is using the right algorithm/data structure

   Other important factors:
   - Clean code - Easy to understand in 1 year or by others.
   - Maintainable code - Easy to expand functionality.
   - Portable code - Easy to re-use it in other systems.

   It is often worth to spend some extra time for these aspects in the beginning 
   so that in the future we may benefit from it. 

   Try to always thinking in parallel in the beginning!
   In worst case scenario, we will run it in our laptop which is 
   still a parallel device. So we can take advantage of this.

** Testing 
   It is good to always test!

   - Unit tests:
     Write functions that perform small tasks that are easy to test. 
     In this way, we can stress each unit of the code. 

   - Integration test:
     All units of code put together work.

   - Systems test:
     Code that was ported works as expected.

** Take advantage of the compiler!
   The compiler is much better than us. It's a fact of life. Our task 
   is to write code that the compiler can easily optimize.
   What is a compiler? 
   A program that translates a program from source language into an equivalent 
   program in a target language. The compiler respects the semantics 
   of our code (more later)
   Compiler stages:
   - precompiler: copies all include statemens into the code
   - front end translates in symbolic language.
   - Assembler: converts to assembly code

   Different compilers have different capabilities so the way we write code helps 
   protect us from these differences. 
   We will look at GNU and PGI compiler. Both are very good in different things. 
   If we write correct code, both yield similar results, but if my code is bad, 
   results could be very different. 

*** Optimization levels of the compiler
    Read manual of compiler - Good starting point.

**** Optimization level: On
     - -On optimize for speed
     - -Os optimized for size

**** Optimization level: native 
     - (!)(Usually neglected) Compile specifically for the specific cpu we are running on.
          In this way, we lose portability, but can gain a lot of performance.
          Use `--march=native` flag for gcc and -xHost in icc compiler

**** Profile guided optimization:
     - Ask compiler to inject in the code, some additional code that logs the 
       patterns of my code. In subsequent compilations, this adittional code will 
       be used to optimize even further. 
       Steps:
     -- gcc `-fprofile-generate`
     -- run the code
     -- gcc `-fprofile-use`

     In other words, the source code remains the same, but the compiled code is 
     different.

**** Qualifiers:
     - const: Declaring a variable as const tells the compiler explicitly that 
       it wont change. This allows the compiler to perform some additional 
       optimization. 
     - volatile: The volatile keyword tells the compiler that the variable will 
       be changed and accessed from outside the program.
     - restrict: Tells the compiler that the memory address is only accessed via 
       the specific pointer. Since this addressed the memory aliasing problem, 
       the compiler can optimize further.
***** Memory Aliasing and the `restrict` keyword
      Memory aliasing occurs when two pointers point to the same memory and we 
      are doing some pointer arithmetics with them. As a quick example:
      @code c 
      // Suppose we have two pointers that point to the same memory region.
      // *a=*b=1
      void add1(int*a, int*b){
          *a+=*b; // both *a=*b=2
          *a+=*b; // both *a=*b=4
      }
      void add2(int*a, int*b){
          *a+= 2* *b; // both *a=*b=3
      }
      @end
      Mathematically speaking, we know that both of the functions above should 
      yield the same results. However, in the case where the pointers point to 
      the same region, we see that we get different results. 

      This is called memory aliasing.

      Since we are not specifying in our code explicitly that the pointers 
      will be pointing to different regions (non overlapping), the compiler 
      cannot convert one form to the other (to optimize), as it cannot know 
      whether we will have the memory aliasing problem. Therefore it will stick 
      to our syntactic code and will not optimize.

      To fix this, we may help the compiler by using the restric keyword, or 
      compile with the `-fstrict-aliasing` `-Wstrict-aliasing` options. The 
      first flag tells the compiler to assume that all pointer point to 
      separate regions, while the second flag asks the compiler to warn us about 
      possible memory aliasing problems.

      @code c 
      // in this way we are telling the compiler that a and b will never overlap 
      // so it is free to optimize as much as possible.
      void add_restrict(int* restrict a, int* restrict b){
          *a+=2* *b;
      }
      @end

** Avoid the avoidable inefficiencies
   - 04 Optimization basic steps
     One of the the main aspects we must care for is to avoid small details 
     that consume extra unecessary cpu cycles. Although seemingly small, these 
     inefficiences accumulate throughtout the execution of our code and can make 
     it less performant. 
*** Guidelines
   - Avoid computing extra, uncessary calculations.

   - Avoid expensive function calls (sqrt, pow function, for example).
    -- Square root function (sqrt):
    --- Prefer to work with squared distances when possible.
    -- Power function (pow):
    --- Prefer to multiply by hand if possible (unless exponent is very large).
    -- Division:
    --- Prefer multiplication instead if possible. For example, if we need to 
        divide by a constant in a for loop, it is more efficient to calculate 
        the inverse of the constant once, and then multiply inside the for loop.

   - Avoid repeated calculations which can be reused. For example, in nested 
     loops, keep track of what variables depend on the loop index so you can 
     extract them to outer levels of the loop and compute them only once. This 
     is called expression `Hoisting`.
    -- Sometimes, we can 'hoist' an expression even further by doing some 
       precomputations what will make our code more efficient.

   - Clarify variable's scope. Local variables should be declared only in their 
     scope. This helps the compiler to organize the stack.

    - Use `register` modifier for variables which are frequently accessed. 
      The `register` modifier suggest to the compiler to keep that variable in 
      the register.

   - Do not assume compiler can always reorganize operations. For example, due 
     to the limited representation capabilities of a machine, floating point 
     operations are not associative. This means the compiler cannot change 
     these operations to make optimizations.
** Take advantage of the CPU architecture
*** Understanding the CPU architecture 
**** Timeline

     - CPU's become much faster than memory.(80's)
     -- A memory hierarchy is introduced to reduce impact. This leads to the 
        concept of *data locality*.

     - CPU's become super scalar (90's).
     -- i.e. instruction level parallelims (ILP) becomes possible.

     - Operations are broken down into smaller stages and pipelined. 
     -- Pipelines can be used to increase throughput.

     - Branch predictors become important part of front end.
     -- Branches cause stalls in pipelines and execution flow. Dealing with this 
        is a key factor to increase performance.

     - CPU's acquire vector capabilities. 
     -- Special registers in the CPU have capabilites to perform the same 
        operation on the same data (SIMD) - `Data level parallelism`. 
** Von Neumann architecture
   One processing unit, one instruction executed at a time, flat memory; i.e. 
   access to memory on any location is the same and access to memory has same 
   cost as instruction.

   Modern architectures are completely different.

** Cache Hierarchy

   Before, cpu's were much faster than memory. This has been partially fixed by 
   ddram but still the main bottleneck is the latency (order of nanoseconds).
   This is in part also caused by physical limitations such as the physical 
   distance that an electrical signal must travel from RAM to the CPU.

   To fix this, manufactures equipped the CPU with a new type of ram (sram) 
   that is much faster and place extremely close to the CPU. However, it is also 
   more energy consuming, more expensive and much smaller than RAM. This memory
   is called the `cache`, and it contains a hierarchy.
   Below is the Hierarchy:
   - Registers
    1 cycle to access
   -- L1 cache ~ 64 Kbytes
      Split in two, one half for data and one half from instructions
      2 cycles to access
   --- L2 cache 
       ~10 cycles to access 
   ---- RAM 
        ~100 cycles to access

   This hierarchy shows the importance of data locality, i.e. having data 
   close to you when needed. 
   Consider for example a system with only an L1 cache and RAM. Then the cost 
   to access memory will be given by:

   %L1hit x L1 cost + %L1miss x RAM cost

   If we have code with 100% L1 hit, the access time will be 2 cycles. 
   But already at 99% L1 hit, we have an average access time of 3 cycles which 
   is 50% more. 
   With 97% we have 5 cycles which is 150% slower. 

   If we work out the math, we see that the introduction of L2 and L3 cache 
   serves to mitigate the effect of L1 cache misses. 

** Data Locality

   Data is defined as "local" when they reside in a small address that is 
   accessed in a short period of time. 

   Local data are more likely to be in the cache.

   There are two principles of locality.

*** Temporal locality

    If the address is reference, it is likely to be referenced again soon.
    This pattern can be observed for example in for loops, where an instruction 
    will be repeated again in the next iteration. 

    This also suggests that it is always useful to break our code into small 
    logical units, so that the data stays in the cache as much as possible. In 
    this case, if we have a ton of instructions inside a for loop, at the next 
    iteration, the data is likely not to be in the cache. So to increase 
    performance, it might be better to somehow split the workflow if possible.

*** Spatial locality 

    If an address is referenced, close addressed are likely to be referenced 
    soon.

    This pattern is seen when we access arrays. If we access one element, we 
    most likely will access also its neighbors soon.

** Cache Mapping

   Question: If RAM is ~O(Gbytes) and L1 cache is ~O(Kbytes), how do we effectively 
   map the RAM to L1 cache?
   The main problems:
   1. Where to map an address
   2. What to do if the location in L1 is already occupied.

*** The 3 C's of memory foes

    ~ Compulsory misses: Unavoidable misses when reading data for the first 
    time.
    ~ Capacity misses: Not enough space to hold all the data or too much data 
      accessed between successive use.
    ~ Conflict misses: Cache trashing due to data mapping to the same cache 
      lines

*** Three R's for friends

    ~ Rearrange(code): Design for spatial and temporal locality.
    ~ Reduce(size): Access smaller chunks and fewer instructions.
    ~ Reuse(cache lines): Increase spatial and temporal locality by doing as much as 
      possible when that data is available.

*** Inside a CPU 

    - Front End - Manager 
    - Back End - Does the work
    -- ports for ILP and Vectorization
    -- Since CPU is superscalar we need to write code so compiler can take 
       advantage of these aspects.

*** Instructions and Pipeline 

    Instructions constist of four stages:
    - fetch: from memory to CPU.
    - decode: must be understood and interpreted.
    - execute: do the instruction.
    - writeback: Write output in memory.

    If four stages take 400 pico-seconds (a.k.a the latency of the instruction),
    we obtain a throughput of 2.5 GIPS (giga operations per second).
    But what if we could "detach" these four stages? 
    We could re-organize everything.

    The idea is that if we have to wait for one instruction to execute, we 
    have to always wait 400 ps. However, if we introduce some specialized units 
    for each of the four stages, as soon as fetching for the first instruction 
    is done, we can immediately start fetching the second instruction while  
    anoter unit for decoding starts to decode the first instruction. 

    This is the basic concept of `pipelining`. The first time around, we have 
    to still wait 400 ps, however, once the pipeline is full, the latency will 
    be reduced and determined by the longest stage. So if we can reduce this, 
    we can have even more throughput.

    So, if we specialize even further by introducing even more specialized units, 
    which perform smaller micro operations, we can break an instruction into 
    even smaller pieces, thus reducing the time of the longest stage.
    This is called `superpipelining` and modern CPU's may have 10-20 stages per 
    pipeline. 

    Taking even further, these specialized units do not necessarily even have 
    to execute in order (as long as it doesnt break the semantics of your code).
    So the CPU may execute many instructions out of order to fill the pipelines 
    to the maximum. At the end of course the results are rearranged to give 
    the correct result.

    Hyperthreading tries to keep the pipelines always busy by sharing the 
    specialized units (specialized ports) among different threads. In other 
    words, if a certain thread is executing some instructions and the pipelines 
    cannot be completely filled due to some semantic dependencies, hyperthreading 
    will ensure that those physical resources that are temporarily free are 
    used by another thread that is also executing at the same time. So it is as 
    if you are doing operations from two threads in the same cycle (kind of 
    doubling the number of cores).

    If your code is extremely well written and takes advantage of pipelines and 
    everything else at the maximum, hyperthreading can give bad results because 
    it can introduce unwanted idle times and interfere with optimal execution. 

*** Vector Registers

    Are large special registers in the CPU that can be considered subdivided in 
    smaller chunks which can perform the _same operation_ on different data (but 
    of the _same type_ i.e all floats or doubles or ints etc...

    This is called `SIMD` single instruction multiple data.

    Vector registers are usually 32 bytes long. So they can contain 4 doubles 
    or 8 floats (ints).

** Cache optimization 

   06 optimization cache

   Memory is much slower than CPU. 
   If our code is so good that the CPU is waiting for memory to be sent, then it 
   is starving for data, and we are in a memory bound regime. 

   In the opposite scenario, we are in a compute bound regime.

*** How is Memory mapped to cache

    The first important concept is that the cache is not reasoning about single 
    bytes but rather chunks of bytes (similar to data alignment). 

    "Let's say that both RAM and cache are subdivided in blocks of equal size, 
    say 64 Bytes. You do not load a single byte in your cache, but an entire 
    block - normally called a line." -- from the course.

    This is actually what happens and you can measure it. In some architectures 
    it is even larger. So a cache line can store 8 doubles.

    The cache usually constains info on: 
    - where the memory is in physical RAM 
    - the data 
    - Some mechanism that signals whether the data is valid or not. 

    This last point is important. Suppose we are multicore and sharing memory. 
    If different threads are working on the same data, then after a thread 
    has modified and written to RAM, the cache in other cores must 
    obtain some signal that that data has been changed so that it can be 
    flushed obtain the latest copy. We will see more about this later. 

*** Cache Mapping (L1 cache)

   This is related to the problem of how to map the RAM which is much bigger 
   to the cache which is orders of magnitude smaller.

**** Full Mapping 

    Data can be placed in any free cache block.
    Convenient for loading, not convenient for accessing data. 

**** Direct Mapping

    Data can only be placed in certain blocks. This can be achieved by modulo 
    arithmetic. 
    Convenient for loading and accessing. But will cause a lot of clashes, 
    because many addresses will be mapped to same place. 

**** N-way associative

    N locations available for a given line. This is a hybrid solution between 
    the two previous approaches and provides a middle ground.
    In fact, the previous two approaches are not used in practice but are useful 
    for understanding. 

    Therefore, in this scenario, the cache is divided into $L1_{size}/N$ /sets/ 
    which hold *N* lines. Addresses usually are mapped (in some way) to a unique 
    set (direct mapping) and then inside the set, we use the first free location 
    (full mapping).

    In modern architectures, a cache line is typically 64 bytes long, and a set 
    usually holds 8 lines. Therefore, we say that it is `8-way associative`.
    ---

   Understanding the mapping is useful to be able to improve the memory access 
   pattern of our program to reduce cache misses and increase hits. 

   This will ultimately increase the throughput of our code.
*** Examples 

    Matrix transposition and multiplication by blocks or by z strided access. 

*** Organizing data to enhance locality

    We study some aspects of how to better organize data to improve data 
    locality and as a consequence, be able to increase cache hits (assuming 
    the code is written well of course).

**** Hot and cold fields
***** Linked list traversal
      Suppose we have a node:
      @code c 
      struct mynode{
          double key;
          char my_data[300];
          my_node* next_node;
      }
      // and some traversal algorithm
      // where we typically access key and next_node
      @end
      During traversal, we will access the key and the next node more often 
      than the data itself (otherwise we would just allocate a huge array for 
      the data).
      With this organizational structure, we will have a cache miss when we 
      access the key, and then 99% of the time, the we will not do anything 
      with the data and want to traverse to the next node. However, since 
      there are 300 bytes of data between key and next_node, next_node will not 
      be in the cache and we will have another cache miss. 

      If we reorganize like this:
      @code c 
      struct mynode{
          double key;
          my_node* next_node;
          char my_data[300];
      }
      @end
      Then, the first access to key will be a cache miss, but the access to 
      next_node will be a hit as it has been loaded to cache.

      This teaches us an important lesson which is: 

      *"What is used together stays together"* 

      So in our case, key and next_node are often used together, therefore, we 
      must put them together in the struct.

      Another optimization is that when we are traversing the linked list, we 
      simply want to identify the right node. We don't really care about the 
      data itself. 
      In terms of efficiency, it is better for us to have structures which are 
      as small as possible, so more can fit in the cache. 

      This leads to the following change:
      @code c 
      struct mynode{
          double key;
          my_node* next_node;
          char * my_data_ptr; // we now point to the data!
      }
      @end

      In this way, many nodes can completely fit into the cache and it makes 
      everything much more efficient if we are traversing the list many times 
      for example.

      In this example, key and next_node are called hot fields, since they are 
      used together often, while my_data is a cold field. We wish group hot 
      fields together and detach cold fields as much as possible, which is what 
      we did with the pointer.
      This whole process is called splitting of hot and cold fields.

* References
  - {https://developer.ibm.com/articles/pa-dalign/}[IBM]
  - Lesson 06 memory allocation
  - 03 Optimization preliminaries and basic compiler usage
  - 04 Optimization Basic Steps
  - 05 Memory architecture
  - 06 optimization cache
