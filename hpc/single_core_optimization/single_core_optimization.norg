* Memory Alignment

  Memory alignment refers to the need to properly place data structures in 
  memory so that the processor may read it efficiently. 

  The first important concept to understand, is that processors do not read from 
  memory in byte sized chunks (as we are accustomed to think about) but rather 
  in multi-byte chunks (depending on your architecture).

  This is referred to as "memory access granularity". In a 32-bit architecture, 
  a processor will read memory in 4 byte chunks (per memory access),
  while in a 64-bit architecture, it will do so in 8 byte chunks.

  Since memory access has an overhead, it is important to reduce it as much as 
  possible to maximize performance (if we care about speed).

  This consideration forces us to think about memory alignment. 

  Suppose we wish are in a 64-bit architecture, and we store a floating point 
  number starting at address 0.

  Since floating point numbers take up 8 bytes, addresses 0-7 will be filled, 
  and to read this number, the processor will need a single memory access.

  However, if we were to store the data beginning at adress 1, the processor 
  will still read a block of 8 bytest starting from adress 0, and then need to 
  read another 8 bytes fromm adress 8 in order to obtain the full representation 
  of the floating point.

  So this requires two memory accesses. In this case, the variable is unaligned.

  The jargon "aligned to X bytes" simply means that we store the value at the 
  appropriate address which is divisible by X. 
  To put it more clearly: An address aligned at 7 bytes means that the address 
  is a multiple of 7. 
  Instead, when we talk about "variable Y aligned at X bytes", we 
  simply use the same concept with variable Y's address.

  So, ideally, we want all our data to be aligned to our systems memory access 
  granularity. 
  In fact, the compiler does this for us automatically when we create new 
  variables, by assigning addresses that are aligned with the data type. 
  This is the reason why, if not careful, a custom data type may take more 
  memory than we expect, since the compiler does some padding to ensure 
  alignment.

* Memory Allocation
  Allocating memory always has an overhead (table maintained, and some other things)
  EXPAND
  - Memory allocation

* Optimization
  - 04 Optimization Basic Steps

** First Steps
   The order of importance:
   - correcteness
   - data model
   - algorithm you choose

   Only after all of this, can we think about optimizing code. 
   You may have beautiful code, but if you're ussing  the wrong 
   algorithm, it will always be worse than a less optimmized code which 
   is using the right algorithm/data structure

   Other important factors:
   - Clean code - Easy to understand in 1 year or by others.
   - Maintainable code - Easy to expand functionality.
   - Portable code - Easy to re-use it in other systems.

   It is often worth to spend some extra time for these aspects in the beginning 
   so that in the future we may benefit from it. 

   Try to always thinking in parallel in the beginning!
   In worst case scenario, we will run it in our laptop which is 
   still a parallel device. So we can take advantage of this.

** Testing 
   It is good to always test!

   - Unit tests:
     Write functions that perform small tasks that are easy to test. 
     In this way, we can stress each unit of the code. 

   - Integration test:
     All units of code put together work.

   - Systems test:
     Code that was ported works as expected.

** Take advantage of the compiler!
   The compiler is much better than us. It's a fact of life. Our task 
   is to write code that the compiler can easily optimize.
   What is a compiler? 
   A program that translates a program from source language into an equivalent 
   program in a target language. The compiler respects the semantics 
   of our code (more later)
   Compiler stages:
   - precompiler: copies all include statemens into the code
   - front end translates in symbolic language.
   - Assembler: converts to assembly code

   Different compilers have different capabilities so the way we write code helps 
   protect us from these differences. 
   We will look at GNU and PGI compiler. Both are very good in different things. 
   If we write correct code, both yield similar results, but if my code is bad, 
   results could be very different. 

*** Optimization levels of the compiler
    Read manual of compiler - Good starting point.

**** Optimization level: On
     - -On optimize for speed
     - -Os optimized for size

**** Optimization level: native 
     - (!)(Usually neglected) Compile specifically for the specific cpu we are running on.
          In this way, we lose portability, but can gain a lot of performance.
          Use `--march=native` flag for gcc and -xHost in icc compiler

**** Profile guided optimization:
     - Ask compiler to inject in the code, some additional code that logs the 
       patterns of my code. In subsequent compilations, this adittional code will 
       be used to optimize even further. 
       Steps:
     -- gcc `-fprofile-generate`
     -- run the code
     -- gcc `-fprofile-use`

     In other words, the source code remains the same, but the compiled code is 
     different.

**** Qualifiers:
     - const: Declaring a variable as const tells the compiler explicitly that 
       it wont change. This allows the compiler to perform some additional 
       optimization. 
     - volatile: The volatile keyword tells the compiler that the variable will 
       be changed and accessed from outside the program.
     - restrict: Tells the compiler that the memory address is only accessed via 
       the specific pointer. Since this addressed the memory aliasing problem, 
       the compiler can optimize further.
***** Memory Aliasing and the `restrict` keyword
      Memory aliasing occurs when two pointers point to the same memory and we 
      are doing some pointer arithmetics with them. As a quick example:
      @code c 
      // Suppose we have two pointers that point to the same memory region.
      // *a=*b=1
      void add1(int*a, int*b){
          *a+=*b; // both *a=*b=2
          *a+=*b; // both *a=*b=4
      }
      void add2(int*a, int*b){
          *a+= 2* *b; // both *a=*b=3
      }
      @end
      Mathematically speaking, we know that both of the functions above should 
      yield the same results. However, in the case where the pointers point to 
      the same region, we see that we get different results. 

      This is called memory aliasing.

      Since we are not specifying in our code explicitly that the pointers 
      will be pointing to different regions (non overlapping), the compiler 
      cannot convert one form to the other (to optimize), as it cannot know 
      whether we will have the memory aliasing problem. Therefore it will stick 
      to our syntactic code and will not optimize.

      To fix this, we may help the compiler by using the restric keyword, or 
      compile with the `-fstrict-aliasing` `-Wstrict-aliasing` options. The 
      first flag tells the compiler to assume that all pointer point to 
      separate regions, while the second flag asks the compiler to warn us about 
      possible memory aliasing problems.

      @code c 
      // in this way we are telling the compiler that a and b will never overlap 
      // so it is free to optimize as much as possible.
      void add_restrict(int* restrict a, int* restrict b){
          *a+=2* *b;
      }
      @end

** Avoid the avoidable inefficiencies
   - 04 Optimization basic steps
     One of the the main aspects we must care for is to avoid small details 
     that consume extra unecessary cpu cycles. Although seemingly small, these 
     inefficiences accumulate throughtout the execution of our code and can make 
     it less performant. 
*** Guidelines
   - Avoid computing extra, uncessary calculations.

   - Avoid expensive function calls (sqrt, pow function, for example).
    -- Square root function (sqrt):
    --- Prefer to work with squared distances when possible.
    -- Power function (pow):
    --- Prefer to multiply by hand if possible (unless exponent is very large).
    -- Division:
    --- Prefer multiplication instead if possible. For example, if we need to 
        divide by a constant in a for loop, it is more efficient to calculate 
        the inverse of the constant once, and then multiply inside the for loop.

   - Avoid repeated calculations which can be reused. For example, in nested 
     loops, keep track of what variables depend on the loop index so you can 
     extract them to outer levels of the loop and compute them only once. This 
     is called expression `Hoisting`.
    -- Sometimes, we can 'hoist' an expression even further by doing some 
       precomputations what will make our code more efficient.

   - Clarify variable's scope. Local variables should be declared only in their 
     scope. This helps the compiler to organize the stack.

    - Use `register` modifier for variables which are frequently accessed. 
      The `register` modifier suggest to the compiler to keep that variable in 
      the register.

   - Do not assume compiler can always reorganize operations. For example, due 
     to the limited representation capabilities of a machine, floating point 
     operations are not associative. This means the compiler cannot change 
     these operations to make optimizations.
** Take advantage of the CPU architecture
*** Understanding the CPU architecture 
**** Timeline

     - CPU's become much faster than memory.(80's)
     -- A memory hierarchy is introduced to reduce impact. This leads to the 
        concept of *data locality*.

     - CPU's become super scalar (90's).
     -- i.e. instruction level parallelims (ILP) becomes possible.

     - Operations are broken down into smaller stages and pipelined. 
     -- Pipelines can be used to increase throughput.

     - Branch predictors become important part of front end.
     -- Branches cause stalls in pipelines and execution flow. Dealing with this 
        is a key factor to increase performance.

     - CPU's acquire vector capabilities. 
     -- Special registers in the CPU have capabilites to perform the same 
        operation on the same data (SIMD) - `Data level parallelism`. 
** Von Neumann architecture
   One processing unit, one instruction executed at a time, flat memory; i.e. 
   access to memory on any location is the same and access to memory has same 
   cost as instruction.

   Modern architectures are completely different.

** Cache Hierarchy
   Before, cpu's were much faster than memory. This has been partially fixed by 
   ddram but still the main bottleneck is the latency (order of nanoseconds).
   This is in part also caused by physical limitations such as the physical 
   distance that an electrical signal must travel from RAM to the CPU.

   To fix this, manufactures equipped the CPU with a new type of ram (sram) 
   that is much faster and place extremely close to the CPU. However, it is also 
   more energy consuming, more expensive and much smaller than RAM. This memory
   is called the `cache`, and it contains a hierarchy.
   Below is the Hierarchy:
   - Registers
    1 cycle to access
   -- L1 cache ~ 64 Kbytes
      Split in two, one half for data and one half from instructions
      2 cycles to access
   --- L2 cache 
       ~10 cycles to access 
   ---- RAM 
        ~100 cycles to access

   This hierarchy shows the importance of data locality, i.e. having data 
   close to you when needed. 
   Consider for example a system with only an L1 cache and RAM. Then the cost 
   to access memory will be given by:

   %L1hit x L1 cost + %L1miss x RAM cost

   If we have code with 100% L1 hit, the access time will be 2 cycles. 
   But already at 99% L1 hit, we have an average access time of 3 cycles which 
   is 50% more. 
   With 97% we have 5 cycles which is 150% slower. 

   If we work out the math, we see that the introduction of L2 and L3 cache 
   serves to mitigate the effect of L1 cache misses. 

** Data Locality
   Data is defined as "local" when they reside in a small address that is 
   accessed in a short period of time. 

   Local data are more likely to be in the cache.

   There are two principles of locality.
*** Temporal locality
    If the address is reference, it is likely to be referenced again soon.
*** Spatial locality 
    If an address is referenced, close addressed are likely to be referenced 
    soon.
** Cache Mapping
   Question: If RAM is ~Gbytes and L1 cache is ~Kbytes, how do we effectively 
   map the RAM to L1 cache?
   The main problems:
   1. Where to map an address
   2. What to do if the location in L1 is already occupied.
*** The 3 C's of memory foes
    ~ Compulsory misses: Unavoidable misses when reading data for the first 
    time.
    ~ Capacity misses: Not enough space to hold all the data or too much data 
      accessed between successive use.
    ~ Conflict misses: Cache trashing due to data mapping to the same cache 
      lines
*** Three R's for friends
    ~ Rearrange(code): Design for spatial and temporal locality.
    ~ Reduce(size): Access smaller chunks and fewer instructions.
    ~ Reuse(cache lines): Increase spatial and temporal locality by doing as much as 
      possible when that data is available.





* References
  - {https://developer.ibm.com/articles/pa-dalign/}[IBM]
  - Lesson 06 memory allocation
  - 03 Optimization preliminaries and basic compiler usage
  - 04 Optimization Basic Steps
  - 05 Memory architecture
